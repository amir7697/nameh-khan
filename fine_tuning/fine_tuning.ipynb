{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e2842d-e5f7-4b12-a79f-1e4237726f7b",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c472ed3-be58-4ea3-9653-3be109c247a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from enum import Enum\n",
    "from itertools import groupby\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from config import MODEL_PARAMS, ENCODING_DICT, DECODING_DICT\n",
    "from misc import HorizontalMirrorImage, ScaleVertically, ScaleHorizontally\n",
    "from model import WordReader\n",
    "from utils import encode_character, decode_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0bee08-199b-468a-a2f5-0b054c8c4352",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcf6868-619b-4154-a19f-b687de0f3c51",
   "metadata": {},
   "source": [
    "### Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07ed93f-9250-478c-bdf0-2cac13d1a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONT = 'zar'\n",
    "\n",
    "PREPROCESSED_IMAGE_PATH = 'dataset/{}/images'.format(FONT)\n",
    "METADATA_PATH = 'dataset/{}/metadata.csv'.format(FONT)\n",
    "\n",
    "CHECKPOINT_DIR = 'models/{}'.format(FONT)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "MODEL_PATH = CHECKPOINT_DIR + '/best_model_accuracy={:.0f}.pth'\n",
    "\n",
    "BASE_MODEL_PATH = '../lib/nameh-khan/nameh_khan/page_reader/plain_text_page/model.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e21991-334e-4640-ba58-21e6c8a0c93f",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1af350-f034-4152-b134-fa9db2305757",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PARAMS = {\n",
    "    'epochs': 100,\n",
    "    'test_size': 0.3,\n",
    "    'learning_rate': 0.0001,\n",
    "    'random_state': 42,\n",
    "    'batch_size': 128,\n",
    "    'lr_schedular_patiance': 5\n",
    "}\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38410c9-0cc4-4e4a-927c-549baacf0854",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26eb210-69bf-4f4e-973a-2a41e0e7cb67",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ed8a2cc-56c3-4b3e-ab5f-388121e6a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_gray_scale(image_path):\n",
    "    return cv.imread(image_path, flags=cv.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb824188-fc83-49de-9b14-e5672478e4f5",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee037e8-5045-4d84-9f7e-2dac12c2cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "def summarize_pdf(df, number_of_rows=3):\n",
    "    if DEBUG:\n",
    "        print('len of df: {}'.format(len(df)))\n",
    "        display(df.head(number_of_rows))\n",
    "\n",
    "class LogLevel(Enum):\n",
    "    info = logging.INFO\n",
    "    error = logging.ERROR\n",
    "\n",
    "def set_log_level(logger, log_level):\n",
    "    logger.setLevel(log_level.value)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "set_log_level(logger, LogLevel.error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa050b-4130-4a6f-9bf1-5010ece8727e",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0ae90b-fd09-4839-ab87-943c0dd828d3",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19550646-cfa5-4c29-b154-4f1cde1da8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in index 113: '30 نفر\n",
      "Error in index 277: '377 نفر فراهم\n",
      "Error in index 2850: '51 نفر\n",
      "Error in index 2857: '88 مورد\n",
      "Error in index 3301: '34 هزار\n",
      "Error in index 3389: '80 میلیارد\n",
      "Error in index 3433: '30 هزار\n",
      "Meta Data Size: 4093\n",
      "Train Size: 2860\n",
      "Validation Size: 1226\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(\n",
    "    processed_image_dir,\n",
    "    metadata_pdf,\n",
    "    alphabet_to_num_dictionary\n",
    "):\n",
    "    dataset = []\n",
    "    for index, row in metadata_pdf.iterrows():\n",
    "        try:\n",
    "            image = read_image_gray_scale(os.path.join(processed_image_dir, '{}.png'.format(row['id'])))\n",
    "            embeded_label = encode_character(row['label'], alphabet_to_num_dictionary)\n",
    "            dataset.append((image, embeded_label))\n",
    "        except Exception as e:\n",
    "            print('Error in index {}: {}'.format(index, row['label']))\n",
    "    return dataset\n",
    "\n",
    "metadata_pdf = pd.read_csv(METADATA_PATH)#.sample(n=DATASET_SIZE, random_state=RANDOM_SEED)\n",
    "dataset = create_dataset(\n",
    "    PREPROCESSED_IMAGE_PATH,\n",
    "    metadata_pdf,\n",
    "    ENCODING_DICT\n",
    ")\n",
    "train_dataset, validation_dataset = train_test_split(\n",
    "    dataset,\n",
    "    test_size=TRAINING_PARAMS['test_size'],\n",
    "    random_state=TRAINING_PARAMS['random_state'],\n",
    "    shuffle=True\n",
    ")\n",
    "print('Meta Data Size: {}'.format(len(metadata_pdf)))\n",
    "print('Train Size: {}'.format(len(train_dataset)))\n",
    "print('Validation Size: {}'.format(len(validation_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26669fb1-3878-4905-a49b-5c9120ca3106",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb0d906b-dd09-4341-90d5-658ff8dc60a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(Dataset):\n",
    "    def __init__(self, dataset, transforms=None):\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        image = image.astype('float32') / 255\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "\n",
    "        image = torch.tensor(image[np.newaxis, ...])\n",
    "        label = torch.Tensor(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca0aedd-d5c2-4be4-963e-6c97ea244ce0",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f686f4-16bc-411e-8382-935d1e9e8025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch: list):\n",
    "    images, labels = map(list, zip(*batch))\n",
    "    padded_images = []\n",
    "    for image in images:\n",
    "        padded_images.append(image[0].unsqueeze(dim=0))\n",
    "\n",
    "    images = torch.cat(padded_images).unsqueeze(dim=1)\n",
    "    labels = nn.utils.rnn.pad_sequence(\n",
    "        sequences=labels,\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    WordDataset(train_dataset, transforms=transforms.Compose([\n",
    "        HorizontalMirrorImage(), \n",
    "        ScaleVertically(desired_height=MODEL_PARAMS['desired_image_height']),\n",
    "        ScaleHorizontally(desired_width=MODEL_PARAMS['desired_image_width'])\n",
    "    ])),\n",
    "    batch_size=TRAINING_PARAMS['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    WordDataset(validation_dataset, transforms=transforms.Compose([\n",
    "        HorizontalMirrorImage(), \n",
    "        ScaleVertically(desired_height=MODEL_PARAMS['desired_image_height']),\n",
    "        ScaleHorizontally(desired_width=MODEL_PARAMS['desired_image_width'])\n",
    "    ])),\n",
    "    batch_size=TRAINING_PARAMS['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690540ce-0b02-41be-83a0-bca1e546ae0f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35906f68-3d80-4da0-844f-932788030c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/.cache/pypoetry/virtualenvs/nameh-khan-I_cqzCEu-py3.9/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = WordReader().to(DEVICE)\n",
    "model.load_state_dict(torch.load(BASE_MODEL_PATH, weights_only=False))\n",
    "criterion = nn.CTCLoss(blank=MODEL_PARAMS['blank_label'], reduction='mean', zero_infinity=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=TRAINING_PARAMS['learning_rate'])\n",
    "learning_rate_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, 'min', verbose=True,\n",
    "    patience=TRAINING_PARAMS['lr_schedular_patiance']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0120b5-85eb-4b85-8833-a10335dd05be",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86e9c100-8e70-497d-84cb-ca10c0887884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_corrects_on_batch(\n",
    "    x, y_pred, y_true, blank=MODEL_PARAMS['blank_label'], \n",
    "    device=DEVICE, decoding_dict=DECODING_DICT\n",
    "):\n",
    "    def check_if_lists_are_same(prediction, y_true):\n",
    "        return prediction == y_true\n",
    "\n",
    "    correct = 0\n",
    "    num = 0\n",
    "    _ ,max_index = torch.max(y_pred, dim=2)\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        raw_prediction = list(max_index[:, i].detach().cpu().numpy())\n",
    "        prediction = torch.IntTensor([c for c, _ in groupby(raw_prediction) if c != blank]).to(device)\n",
    "\n",
    "        decoded_prediction = decode_text(prediction, decoding_dict).replace(' ', '')\n",
    "        decoded_label = decode_text(y_true[i].detach().cpu().numpy(), decoding_dict).replace(' ', '')\n",
    "\n",
    "        logging.info(f\"Decoded Prediction: {''.join(decoded_prediction)}\")\n",
    "        logging.info(f\"Label:              {''.join(decoded_label)}\")\n",
    "\n",
    "        if check_if_lists_are_same(decoded_prediction, decoded_label):\n",
    "            correct += 1\n",
    "        num += 1\n",
    "\n",
    "    return correct, num\n",
    "    \n",
    "\n",
    "def iterate_over_full_dataset(model, data_loader, device, criterion, is_training, optimizer):\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    total_correct = 0\n",
    "    total_num = 0\n",
    "\n",
    "    for x, y in data_loader:\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.permute(1, 0 ,2)\n",
    "\n",
    "        input_lengths = torch.IntTensor(batch_size).fill_(y_pred.shape[0])\n",
    "        target_lengths = torch.IntTensor([len(t) for t in y])\n",
    "\n",
    "        if is_training:\n",
    "            loss = criterion(y_pred, y, input_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                loss = criterion(y_pred, y, input_lengths, target_lengths)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "\n",
    "        correct, num = count_corrects_on_batch(x, y_pred, y)\n",
    "        total_correct += correct\n",
    "        total_num += num\n",
    "\n",
    "    return {'accuracy': round(total_correct/total_num, 3), 'loss': round(total_loss/batch_count, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2443a020-846f-473a-8ecf-eeca312755ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Info: {'accuracy': 0.595, 'loss': 0.527}\n",
      "Val Info: {'accuracy': 0.684, 'loss': 0.432}\n",
      "Epoch: 2\n",
      "Train Info: {'accuracy': 0.717, 'loss': 0.407}\n",
      "Val Info: {'accuracy': 0.74, 'loss': 0.383}\n",
      "Epoch: 3\n",
      "Train Info: {'accuracy': 0.785, 'loss': 0.344}\n",
      "Val Info: {'accuracy': 0.772, 'loss': 0.363}\n",
      "Epoch: 4\n",
      "Train Info: {'accuracy': 0.821, 'loss': 0.297}\n",
      "Val Info: {'accuracy': 0.787, 'loss': 0.317}\n",
      "Epoch: 5\n",
      "Train Info: {'accuracy': 0.85, 'loss': 0.284}\n",
      "Val Info: {'accuracy': 0.807, 'loss': 0.318}\n",
      "Epoch: 6\n",
      "Train Info: {'accuracy': 0.873, 'loss': 0.277}\n",
      "Val Info: {'accuracy': 0.818, 'loss': 0.296}\n",
      "Epoch: 7\n",
      "Train Info: {'accuracy': 0.888, 'loss': 0.275}\n",
      "Val Info: {'accuracy': 0.83, 'loss': 0.317}\n",
      "Epoch: 8\n",
      "Train Info: {'accuracy': 0.908, 'loss': 0.262}\n",
      "Val Info: {'accuracy': 0.835, 'loss': 0.274}\n",
      "Epoch: 9\n",
      "Train Info: {'accuracy': 0.917, 'loss': 0.262}\n",
      "Val Info: {'accuracy': 0.844, 'loss': 0.287}\n",
      "Epoch: 10\n",
      "Train Info: {'accuracy': 0.926, 'loss': 0.229}\n",
      "Val Info: {'accuracy': 0.846, 'loss': 0.282}\n",
      "Epoch: 11\n",
      "Train Info: {'accuracy': 0.935, 'loss': 0.217}\n",
      "Val Info: {'accuracy': 0.851, 'loss': 0.264}\n",
      "Epoch: 12\n",
      "Train Info: {'accuracy': 0.939, 'loss': 0.222}\n",
      "Val Info: {'accuracy': 0.852, 'loss': 0.263}\n",
      "Epoch: 13\n",
      "Train Info: {'accuracy': 0.943, 'loss': 0.21}\n",
      "Val Info: {'accuracy': 0.859, 'loss': 0.251}\n",
      "Epoch: 14\n",
      "Train Info: {'accuracy': 0.948, 'loss': 0.198}\n",
      "Val Info: {'accuracy': 0.863, 'loss': 0.234}\n",
      "Epoch: 15\n",
      "Train Info: {'accuracy': 0.953, 'loss': 0.206}\n",
      "Val Info: {'accuracy': 0.87, 'loss': 0.247}\n",
      "Epoch: 16\n",
      "Train Info: {'accuracy': 0.962, 'loss': 0.185}\n",
      "Val Info: {'accuracy': 0.87, 'loss': 0.261}\n",
      "Epoch: 17\n",
      "Train Info: {'accuracy': 0.965, 'loss': 0.185}\n",
      "Val Info: {'accuracy': 0.874, 'loss': 0.27}\n",
      "Epoch: 18\n",
      "Train Info: {'accuracy': 0.969, 'loss': 0.202}\n",
      "Val Info: {'accuracy': 0.876, 'loss': 0.237}\n",
      "Epoch: 19\n",
      "Train Info: {'accuracy': 0.972, 'loss': 0.2}\n",
      "Val Info: {'accuracy': 0.878, 'loss': 0.233}\n",
      "Epoch: 20\n",
      "Train Info: {'accuracy': 0.973, 'loss': 0.2}\n",
      "Val Info: {'accuracy': 0.883, 'loss': 0.23}\n",
      "Epoch: 21\n",
      "Train Info: {'accuracy': 0.976, 'loss': 0.198}\n",
      "Val Info: {'accuracy': 0.883, 'loss': 0.239}\n",
      "Epoch: 22\n",
      "Train Info: {'accuracy': 0.978, 'loss': 0.185}\n",
      "Val Info: {'accuracy': 0.887, 'loss': 0.246}\n",
      "Epoch: 23\n",
      "Train Info: {'accuracy': 0.978, 'loss': 0.188}\n",
      "Val Info: {'accuracy': 0.887, 'loss': 0.235}\n",
      "Epoch: 24\n",
      "Train Info: {'accuracy': 0.98, 'loss': 0.191}\n",
      "Val Info: {'accuracy': 0.888, 'loss': 0.225}\n",
      "Epoch: 25\n",
      "Train Info: {'accuracy': 0.983, 'loss': 0.191}\n",
      "Val Info: {'accuracy': 0.891, 'loss': 0.215}\n",
      "Epoch: 26\n",
      "Train Info: {'accuracy': 0.984, 'loss': 0.189}\n",
      "Val Info: {'accuracy': 0.893, 'loss': 0.222}\n",
      "Epoch: 27\n",
      "Train Info: {'accuracy': 0.985, 'loss': 0.177}\n",
      "Val Info: {'accuracy': 0.892, 'loss': 0.226}\n",
      "Epoch: 28\n",
      "Train Info: {'accuracy': 0.986, 'loss': 0.181}\n",
      "Val Info: {'accuracy': 0.893, 'loss': 0.222}\n",
      "Epoch: 29\n",
      "Train Info: {'accuracy': 0.987, 'loss': 0.183}\n",
      "Val Info: {'accuracy': 0.894, 'loss': 0.222}\n",
      "Epoch: 30\n",
      "Train Info: {'accuracy': 0.987, 'loss': 0.19}\n",
      "Val Info: {'accuracy': 0.9, 'loss': 0.227}\n",
      "Epoch: 31\n",
      "Train Info: {'accuracy': 0.988, 'loss': 0.177}\n",
      "Val Info: {'accuracy': 0.903, 'loss': 0.218}\n",
      "Epoch: 32\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.195}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.223}\n",
      "Epoch: 33\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.183}\n",
      "Val Info: {'accuracy': 0.903, 'loss': 0.247}\n",
      "Epoch: 34\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.184}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.236}\n",
      "Epoch: 35\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.177}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.222}\n",
      "Epoch: 36\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.188}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.215}\n",
      "Epoch: 37\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.186}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.222}\n",
      "Epoch: 38\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.177}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.225}\n",
      "Epoch: 39\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.211}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.233}\n",
      "Epoch: 40\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.173}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.223}\n",
      "Epoch: 41\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.208}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.27}\n",
      "Epoch: 42\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.178}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.23}\n",
      "Epoch: 43\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.188}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.227}\n",
      "Epoch: 44\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.201}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.228}\n",
      "Epoch: 45\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.193}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.236}\n",
      "Epoch: 46\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.195}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.227}\n",
      "Epoch: 47\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.172}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.22}\n",
      "Epoch: 48\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.181}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.229}\n",
      "Epoch: 49\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.185}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.235}\n",
      "Epoch: 50\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.183}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.269}\n",
      "Epoch: 51\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.178}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.248}\n",
      "Epoch: 52\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.18}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.247}\n",
      "Epoch: 53\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.177}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.225}\n",
      "Epoch: 54\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.187}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.24}\n",
      "Epoch: 55\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.197}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.227}\n",
      "Epoch: 56\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.182}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.228}\n",
      "Epoch: 57\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.176}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.215}\n",
      "Epoch: 58\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.188}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.229}\n",
      "Epoch: 59\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.202}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.211}\n",
      "Epoch: 60\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.188}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.225}\n",
      "Epoch: 61\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.189}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.243}\n",
      "Epoch: 62\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.191}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.203}\n",
      "Epoch: 63\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.183}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.227}\n",
      "Epoch: 64\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.189}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.265}\n",
      "Epoch: 65\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.181}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.229}\n",
      "Epoch: 66\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.188}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.243}\n",
      "Epoch: 67\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.21}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.226}\n",
      "Epoch: 68\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.171}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.223}\n",
      "Epoch: 69\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.178}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.281}\n",
      "Epoch: 70\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.179}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.216}\n",
      "Epoch: 71\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.185}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.224}\n",
      "Epoch: 72\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.192}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.249}\n",
      "Epoch: 73\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.178}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.219}\n",
      "Epoch: 74\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.184}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.213}\n",
      "Epoch: 75\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.181}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.229}\n",
      "Epoch: 76\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.196}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.213}\n",
      "Epoch: 77\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.193}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.267}\n",
      "Epoch: 78\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.191}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.264}\n",
      "Epoch: 79\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.175}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.244}\n",
      "Epoch: 80\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.195}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.217}\n",
      "Epoch: 81\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.179}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.248}\n",
      "Epoch: 82\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.17}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.224}\n",
      "Epoch: 83\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.185}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.222}\n",
      "Epoch: 84\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.183}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.218}\n",
      "Epoch: 85\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.168}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.223}\n",
      "Epoch: 86\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.198}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.244}\n",
      "Epoch: 87\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.196}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.22}\n",
      "Epoch: 88\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.19}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.253}\n",
      "Epoch: 89\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.178}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.235}\n",
      "Epoch: 90\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.201}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.225}\n",
      "Epoch: 91\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.186}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.214}\n",
      "Epoch: 92\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.182}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.221}\n",
      "Epoch: 93\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.176}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.227}\n",
      "Epoch: 94\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.179}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.219}\n",
      "Epoch: 95\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.178}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.236}\n",
      "Epoch: 96\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.19}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.233}\n",
      "Epoch: 97\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.173}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.22}\n",
      "Epoch: 98\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.222}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.247}\n",
      "Epoch: 99\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.18}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.238}\n",
      "Epoch: 100\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.177}\n",
      "Val Info: {'accuracy': 0.902, 'loss': 0.221}\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "train_accs = []\n",
    "validation_accs = []\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "for i in range(TRAINING_PARAMS['epochs']):\n",
    "    print('Epoch: {}'.format(i+1))\n",
    "    train_info = iterate_over_full_dataset(\n",
    "        model,\n",
    "        data_loader=train_dataloader,\n",
    "        device=DEVICE,\n",
    "        criterion=criterion,\n",
    "        is_training=True,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "    validation_info = iterate_over_full_dataset(\n",
    "        model,\n",
    "        data_loader=validation_dataloader,\n",
    "        device=DEVICE,\n",
    "        criterion=criterion,\n",
    "        is_training=False,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "\n",
    "    learning_rate_scheduler.step(validation_info['loss'])\n",
    "\n",
    "    train_losses.append(train_info['loss'])\n",
    "    train_accs.append(train_info['accuracy'])\n",
    "\n",
    "    validation_losses.append(validation_info['loss'])\n",
    "    validation_accs.append(validation_info['accuracy'])\n",
    "\n",
    "    print('Train Info: {}'.format(train_info))\n",
    "    print('Val Info: {}'.format(validation_info))\n",
    "\n",
    "    if best_val_acc < validation_info['accuracy']:\n",
    "        best_val_acc = validation_info['accuracy']\n",
    "        torch.save(model.state_dict(), MODEL_PATH.format(100*best_val_acc))\n",
    "        torch.save(model.state_dict(), 'models/best_model_{}.pth'.format(FONT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
