{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e562ddf-3b74-4ee5-8aa2-ddd78c1d2a6a",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e116b091-51db-4a54-88a1-81bf358ef50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from enum import Enum\n",
    "from itertools import groupby\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from config import MODEL_PARAMS, ENCODING_DICT, DECODING_DICT\n",
    "from misc import HorizontalMirrorImage, ScaleVertically, ScaleHorizontally\n",
    "from model import WordReader\n",
    "from utils import encode_character, decode_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a187e859-7cf1-4831-bfb2-ad397e34c422",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f5c12-7a17-4705-9a6e-406e260ba6d1",
   "metadata": {},
   "source": [
    "## Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b5b207-cf66-4d97-81ba-e4fc03e31036",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_IMAGE_PATH = 'augmented-data/images'\n",
    "METADATA_PATH = 'augmented-data/metadata.csv'\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "MODEL_PATH = 'models/best_model_accuracy={:.0f}.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21c295-ede7-4412-84e8-ae7cba578496",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ab86ee-73ed-4f82-8a71-048f993b13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PARAMS = {\n",
    "    'epochs': 15,\n",
    "    'test_size': 0.2,\n",
    "    'learning_rate': 0.001,\n",
    "    'random_state': 42,\n",
    "    'batch_size': 128,\n",
    "    'lr_schedular_patiance': 5\n",
    "}\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DATASET_SIZE = 1_000_000\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21782e2c-4d2e-45f4-a1bd-e31d9ced4f74",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5587b0-d557-45af-8ab7-bb7279af6748",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3d7c53-f810-449e-aa17-a7de353d9cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_gray_scale(image_path):\n",
    "    return cv.imread(image_path, flags=cv.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c750a-68ae-4342-8705-74a8e0f4a60d",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8964c829-cb87-40a3-941b-7ce292a1f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "def summarize_pdf(df, number_of_rows=3):\n",
    "    if DEBUG:\n",
    "        print('len of df: {}'.format(len(df)))\n",
    "        display(df.head(number_of_rows))\n",
    "\n",
    "class LogLevel(Enum):\n",
    "    info = logging.INFO\n",
    "    error = logging.ERROR\n",
    "\n",
    "def set_log_level(logger, log_level):\n",
    "    logger.setLevel(log_level.value)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "set_log_level(logger, LogLevel.error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdbd203-5fbc-48d5-9425-b8a0be907906",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f9b49-080a-4b57-99d2-56603e5a9ba6",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "990d2425-4d76-47a8-8fea-f034de938507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in index 212727: آیۀ\n",
      "Error in index 212778: وظیفۀ\n",
      "Error in index 212836: مطالبۀ\n",
      "Error in index 212928: وظیفۀ\n",
      "Error in index 213013: جایزۀ\n",
      "Error in index 213176: خلاصۀ\n",
      "Error in index 245836: بازماندۀ\n",
      "Error in index 257982: پدیدۀ\n",
      "Error in index 257986: استفادۀ\n",
      "Error in index 258022: ارائۀ\n",
      "Error in index 258150: حادثۀ\n",
      "Error in index 509472: جشنوارۀ\n",
      "Error in index 509485: دورۀ\n",
      "Error in index 509486: جشنوارۀ\n",
      "Error in index 509542: مرحلۀ\n",
      "Error in index 509569: جشنوارۀ\n",
      "Error in index 509604: نتیجۀ\n",
      "Error in index 540580: فاصلۀ\n",
      "Error in index 540665: جامعۀ\n",
      "Error in index 540726: فاصلۀ\n",
      "Error in index 540767: عمدۀ\n",
      "Error in index 540808: تجربۀ\n",
      "Error in index 540822: استفادۀ\n",
      "Error in index 540878: دربارۀ\n",
      "Error in index 540893: برنامۀ\n",
      "Error in index 540964: داد:برنامۀ\n",
      "Error in index 540996: مجموعۀ\n",
      "Error in index 642535: مرحلۀ\n",
      "Error in index 642580: هزینۀچاپ\n",
      "Error in index 642739: مجموعۀ\n",
      "Error in index 779468: ارایۀ\n",
      "Error in index 779475: دورۀ\n",
      "Error in index 779485: بودجۀ\n",
      "Error in index 779504: بودجۀ\n",
      "Error in index 779529: بودجۀ\n",
      "Error in index 779549: بودجۀ\n",
      "Error in index 779563: بودجۀ\n",
      "Error in index 779588: بودجۀ\n",
      "Error in index 779617: بودجۀ\n",
      "Error in index 779724: دورۀ\n",
      "Error in index 779801: بستۀ\n",
      "Error in index 779809: بستۀ\n",
      "Error in index 779876: خیریۀ\n",
      "Error in index 779926: ویژه‌برنامۀ\n",
      "Error in index 780002: دربارۀ\n",
      "Error in index 780003: پروژۀ\n",
      "Error in index 780015: شش‌دروازۀ\n",
      "Error in index 780020: پروژۀ\n",
      "Error in index 780053: زمینۀ\n",
      "Error in index 809667: نتیجۀ\n",
      "Error in index 809724: نتیجۀ\n",
      "Error in index 833244: همۀ\n",
      "Error in index 836968: سرسپردۀ\n",
      "Error in index 836989: مسألۀ\n",
      "Error in index 837104: نتیجۀ\n",
      "Error in index 837168: پرودۀ\n",
      "Error in index 837187: پروژۀ\n",
      "Error in index 837233: شدۀ\n",
      "Error in index 946959: منطقۀ\n",
      "Error in index 1008520: زمینۀ\n",
      "Error in index 1008766: فیروزۀ\n",
      "Error in index 1008772: حوزۀ\n",
      "Error in index 1008875: نمایندۀ\n",
      "Error in index 1008897: نمایندۀ\n",
      "Meta Data Size: 1117351\n",
      "Train Size: 893829\n",
      "Validation Size: 223458\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(\n",
    "    processed_image_dir,\n",
    "    metadata_pdf,\n",
    "    alphabet_to_num_dictionary\n",
    "):\n",
    "    dataset = []\n",
    "    for index, row in metadata_pdf.iterrows():\n",
    "        try:\n",
    "            image = read_image_gray_scale(os.path.join(processed_image_dir, '{}.png'.format(row['id'])))\n",
    "            embeded_label = encode_character(row['label'], alphabet_to_num_dictionary)\n",
    "            dataset.append((image, embeded_label))\n",
    "        except Exception as e:\n",
    "            print('Error in index {}: {}'.format(index, row['label']))\n",
    "    return dataset\n",
    "\n",
    "metadata_pdf = pd.read_csv(METADATA_PATH)#.sample(n=DATASET_SIZE, random_state=RANDOM_SEED)\n",
    "dataset = create_dataset(\n",
    "    PREPROCESSED_IMAGE_PATH,\n",
    "    metadata_pdf,\n",
    "    ENCODING_DICT\n",
    ")\n",
    "train_dataset, validation_dataset = train_test_split(\n",
    "    dataset,\n",
    "    test_size=TRAINING_PARAMS['test_size'],\n",
    "    random_state=TRAINING_PARAMS['random_state'],\n",
    "    shuffle=True\n",
    ")\n",
    "print('Meta Data Size: {}'.format(len(metadata_pdf)))\n",
    "print('Train Size: {}'.format(len(train_dataset)))\n",
    "print('Validation Size: {}'.format(len(validation_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003125e-5906-40ec-9e17-ebf4244d3f06",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e5c3f17-d51f-4375-8e6f-f81f0fe723e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(Dataset):\n",
    "    def __init__(self, dataset, transforms=None):\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        image = image.astype('float32') / 255\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "\n",
    "        image = torch.tensor(image[np.newaxis, ...])\n",
    "        label = torch.Tensor(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce32ce-bc82-4411-87ac-0e7e6eac1aa2",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d183e1da-2ace-4e32-ab79-5a26c941c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch: list):\n",
    "    images, labels = map(list, zip(*batch))\n",
    "    padded_images = []\n",
    "    for image in images:\n",
    "        padded_images.append(image[0].unsqueeze(dim=0))\n",
    "\n",
    "    images = torch.cat(padded_images).unsqueeze(dim=1)\n",
    "    labels = nn.utils.rnn.pad_sequence(\n",
    "        sequences=labels,\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    WordDataset(train_dataset, transforms=transforms.Compose([\n",
    "        HorizontalMirrorImage(), \n",
    "        ScaleVertically(desired_height=MODEL_PARAMS['desired_image_height']),\n",
    "        ScaleHorizontally(desired_width=MODEL_PARAMS['desired_image_width'])\n",
    "    ])),\n",
    "    batch_size=TRAINING_PARAMS['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    WordDataset(validation_dataset, transforms=transforms.Compose([\n",
    "        HorizontalMirrorImage(), \n",
    "        ScaleVertically(desired_height=MODEL_PARAMS['desired_image_height']),\n",
    "        ScaleHorizontally(desired_width=MODEL_PARAMS['desired_image_width'])\n",
    "    ])),\n",
    "    batch_size=TRAINING_PARAMS['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d68de-b6a5-4ee3-b848-da2d23c704e3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a21f447-ceb7-4835-bfb8-b226c3f4f5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/.cache/pypoetry/virtualenvs/nameh-khan-I_cqzCEu-py3.9/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = WordReader().to(DEVICE)\n",
    "criterion = nn.CTCLoss(blank=MODEL_PARAMS['blank_label'], reduction='mean', zero_infinity=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=TRAINING_PARAMS['learning_rate'])\n",
    "learning_rate_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, 'min', verbose=True,\n",
    "    patience=TRAINING_PARAMS['lr_schedular_patiance']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e40990-3be7-44a3-820f-435b43544176",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1997af7-f445-40d6-8122-cbed390e0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_corrects_on_batch(\n",
    "    x, y_pred, y_true, blank=MODEL_PARAMS['blank_label'], \n",
    "    device=DEVICE, decoding_dict=DECODING_DICT\n",
    "):\n",
    "    def check_if_lists_are_same(prediction, y_true):\n",
    "        return prediction == y_true\n",
    "\n",
    "    correct = 0\n",
    "    num = 0\n",
    "    _ ,max_index = torch.max(y_pred, dim=2)\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        raw_prediction = list(max_index[:, i].detach().cpu().numpy())\n",
    "        prediction = torch.IntTensor([c for c, _ in groupby(raw_prediction) if c != blank]).to(device)\n",
    "\n",
    "        decoded_prediction = decode_text(prediction, decoding_dict).replace(' ', '')\n",
    "        decoded_label = decode_text(y_true[i].detach().cpu().numpy(), decoding_dict).replace(' ', '')\n",
    "\n",
    "        logging.info(f\"Decoded Prediction: {''.join(decoded_prediction)}\")\n",
    "        logging.info(f\"Label:              {''.join(decoded_label)}\")\n",
    "\n",
    "        if check_if_lists_are_same(decoded_prediction, decoded_label):\n",
    "            correct += 1\n",
    "        num += 1\n",
    "\n",
    "    return correct, num\n",
    "    \n",
    "\n",
    "def iterate_over_full_dataset(model, data_loader, device, criterion, is_training, optimizer):\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    total_correct = 0\n",
    "    total_num = 0\n",
    "\n",
    "    for x, y in data_loader:\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.permute(1, 0 ,2)\n",
    "\n",
    "        input_lengths = torch.IntTensor(batch_size).fill_(y_pred.shape[0])\n",
    "        target_lengths = torch.IntTensor([len(t) for t in y])\n",
    "\n",
    "        if is_training:\n",
    "            loss = criterion(y_pred, y, input_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                loss = criterion(y_pred, y, input_lengths, target_lengths)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "\n",
    "        correct, num = count_corrects_on_batch(x, y_pred, y)\n",
    "        total_correct += correct\n",
    "        total_num += num\n",
    "\n",
    "    return {'accuracy': round(total_correct/total_num, 3), 'loss': round(total_loss/batch_count, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc2b96cf-03c5-41aa-8a4a-fc67aac40076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Info: {'accuracy': 0.721, 'loss': 0.616}\n",
      "Val Info: {'accuracy': 0.944, 'loss': 0.225}\n",
      "Epoch: 2\n",
      "Train Info: {'accuracy': 0.958, 'loss': 0.2}\n",
      "Val Info: {'accuracy': 0.968, 'loss': 0.182}\n",
      "Epoch: 3\n",
      "Train Info: {'accuracy': 0.975, 'loss': 0.184}\n",
      "Val Info: {'accuracy': 0.974, 'loss': 0.179}\n",
      "Epoch: 4\n",
      "Train Info: {'accuracy': 0.981, 'loss': 0.18}\n",
      "Val Info: {'accuracy': 0.982, 'loss': 0.176}\n",
      "Epoch: 5\n",
      "Train Info: {'accuracy': 0.985, 'loss': 0.178}\n",
      "Val Info: {'accuracy': 0.986, 'loss': 0.183}\n",
      "Epoch: 6\n",
      "Train Info: {'accuracy': 0.987, 'loss': 0.179}\n",
      "Val Info: {'accuracy': 0.983, 'loss': 0.177}\n",
      "Epoch: 7\n",
      "Train Info: {'accuracy': 0.988, 'loss': 0.177}\n",
      "Val Info: {'accuracy': 0.987, 'loss': 0.175}\n",
      "Epoch: 8\n",
      "Train Info: {'accuracy': 0.988, 'loss': 0.176}\n",
      "Val Info: {'accuracy': 0.986, 'loss': 0.174}\n",
      "Epoch: 9\n",
      "Train Info: {'accuracy': 0.989, 'loss': 0.176}\n",
      "Val Info: {'accuracy': 0.987, 'loss': 0.173}\n",
      "Epoch: 10\n",
      "Train Info: {'accuracy': 0.989, 'loss': 0.174}\n",
      "Val Info: {'accuracy': 0.989, 'loss': 0.173}\n",
      "Epoch: 11\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.174}\n",
      "Val Info: {'accuracy': 0.988, 'loss': 0.177}\n",
      "Epoch: 12\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.174}\n",
      "Val Info: {'accuracy': 0.986, 'loss': 0.176}\n",
      "Epoch: 13\n",
      "Train Info: {'accuracy': 0.99, 'loss': 0.175}\n",
      "Val Info: {'accuracy': 0.99, 'loss': 0.174}\n",
      "Epoch: 14\n",
      "Train Info: {'accuracy': 0.989, 'loss': 0.176}\n",
      "Val Info: {'accuracy': 0.989, 'loss': 0.173}\n",
      "Epoch: 15\n",
      "Train Info: {'accuracy': 0.991, 'loss': 0.174}\n",
      "Val Info: {'accuracy': 0.99, 'loss': 0.18}\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "train_accs = []\n",
    "validation_accs = []\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "for i in range(TRAINING_PARAMS['epochs']):\n",
    "    print('Epoch: {}'.format(i+1))\n",
    "    train_info = iterate_over_full_dataset(\n",
    "        model,\n",
    "        data_loader=train_dataloader,\n",
    "        device=DEVICE,\n",
    "        criterion=criterion,\n",
    "        is_training=True,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "    validation_info = iterate_over_full_dataset(\n",
    "        model,\n",
    "        data_loader=validation_dataloader,\n",
    "        device=DEVICE,\n",
    "        criterion=criterion,\n",
    "        is_training=False,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "\n",
    "    learning_rate_scheduler.step(validation_info['loss'])\n",
    "\n",
    "    train_losses.append(train_info['loss'])\n",
    "    train_accs.append(train_info['accuracy'])\n",
    "\n",
    "    validation_losses.append(validation_info['loss'])\n",
    "    validation_accs.append(validation_info['accuracy'])\n",
    "\n",
    "    print('Train Info: {}'.format(train_info))\n",
    "    print('Val Info: {}'.format(validation_info))\n",
    "\n",
    "    if best_val_acc < validation_info['accuracy']:\n",
    "        best_val_acc = validation_info['accuracy']\n",
    "        torch.save(model.state_dict(), MODEL_PATH.format(100*best_val_acc))\n",
    "        torch.save(model.state_dict(), 'models/best_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
